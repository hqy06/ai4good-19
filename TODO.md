# Models

## K Nearest Neighbor

- [x] Note
- [x] Write code
- [x] Debug using IRIS
- [ ] More test

## K Means

- [x] Note
- [x] Write Code
- [x] Debug using WINE
- [ ] Implement Soft K-Means
- [ ] Debug using datasets

Try use the precision matrix, i.e. $(a_{ij})$ in which each row is the result of clustering prediction and each column for the actual label.
Also, check [MNIST 2d visualization](https://colah.github.io/posts/2014-10-Visualizing-MNIST/)

Bishop: life time good book to read

Read later: [link](https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py)

## MLP: fully-connected feed-forward NN

- [x] Note
- [x] Simple MLP: no debug!
- [ ] MLP in `sklearn`
- [ ] MLP in `pytorch`

## CNN: convolution neural net

- [ ] Note
- [ ] Simple Code
- [ ] TBA

## RNN: recurrent neural net

- [ ] Note
- [ ] Simple Code
- [ ] TBA

---

# Talks

## Reinforce Learning

- [ ] Prof. Precup: day 2
- [ ] Prof. Precup: day 3
- [ ] TBA

## Monte Carlo Tree Search

- [ ] TBA

## Natural Language Processing

- [ ] TBA
